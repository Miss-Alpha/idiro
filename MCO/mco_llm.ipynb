{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c14a511-797d-4cd3-ad8e-dca217ef5115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0d9f4-6970-4ef0-b229-c29402bcf35f",
   "metadata": {},
   "source": [
    "# 0. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bc0e2-1834-4c4d-aa0f-6b6fcc53a36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_community\n",
    "!pip install -q unstructured # for directory_reader\n",
    "!pip install -q unstructured[xlsx] # for reading xlsx files\n",
    "!pip install -q gradio\n",
    "!pip install -q qdrant-client\n",
    "!pip install -q oci==2.112.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ee526-fa95-4304-a7d7-10693c2209d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"unstructured[xlsx]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043281e-0697-48dc-93fc-a683504a3384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b3313-b325-49c4-88d2-07018a007415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9753b61-0abe-45a1-b2c6-5b3d0000b1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a33f51-0ac8-41fb-8f22-e646f284f777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import oci\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from my_directory_loader import MyDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #for directory_reader\n",
    "from langchain.vectorstores import Qdrant\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391aa204-1fef-432f-9729-d54a1375b2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compartment_id = os.environ[\"NB_SESSION_COMPARTMENT_OCID\"]\n",
    "service_endpoint = \"https://inference.generativeai.eu-frankfurt-1.oci.oraclecloud.com\"\n",
    "oci_signer = oci.auth.signers.get_resource_principals_signer() # this has changed from its original version to resource principal, do not forget to use this\n",
    "#oci_signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner() #preferred way but policies needs to be in place to enable instance principal or resource principal https://docs.oracle.com/en-us/iaas/data-flow/using/resource-principal-policies.htm\n",
    "DIRECTORY = 'data'\n",
    "NEW_DIRECTORY = 'new_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddefe6e8-f501-4e8e-8774-6aaf03194ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ocid1.compartment.oc1..aaaaaaaa2mybnl25h6pdqqicnuog4gzfrcdyjjwmpq3246nklfsv5s3obdkq'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compartment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3603ac7c-0312-4140-84cb-ed3d9439dcfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-english-v3.0\",\n",
    "    service_endpoint=\"https://inference.generativeai.eu-frankfurt-1.oci.oraclecloud.com\",\n",
    "    compartment_id= compartment_id,\n",
    "    auth_type=\"RESOURCE_PRINCIPAL\", # this is used for serverless architecture like this env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343e77b2-645a-4c66-bb87-9065fe5af2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_test = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-multilingual-v3.0\",\n",
    "    service_endpoint=service_endpoint,\n",
    "    compartment_id=compartment_id,\n",
    "    # auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "    auth_type=\"RESOURCE_PRINCIPAL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cee6c88-ceaf-46bd-85f3-8dea6c3d00c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  loader = MyDirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "documents = load_docs(NEW_DIRECTORY)\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "#def split_docs(documents,chunk_size=256,chunk_overlap=20): # second try\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "docs = split_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d4b59-e572-4bac-a8ac-77ce9961e9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53144aba-917e-42b6-b23b-ba4bb93f8056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similiar_docs(query,k=4,score=False):\n",
    "  if score:\n",
    "    similar_docs = db.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a62162-dbd2-4331-8e11-cbf73d7dfc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = Qdrant.from_documents(docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30c1fec6-ddba-4d71-ab55-65521a61d8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_context = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12c94d-0717-413b-bc95-0c698f6c43fd",
   "metadata": {},
   "source": [
    "# 1. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa899beb-96e8-42cf-bb13-a704751a6fa4",
   "metadata": {},
   "source": [
    "## 1.1. Implementation with New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cac32e5-b8d1-47a8-a550-b59533097165",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRELAND_DIRECTORY = \"new_data/Ireland\"\n",
    "US_DIRECTORY = \"new_data/USA\"\n",
    "\n",
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f56afbc3-3a55-45b2-a2c9-c8dbe0777009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  loader = MyDirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "ireland_documents = load_docs(IRELAND_DIRECTORY)\n",
    "us_documents = load_docs(US_DIRECTORY)\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "#def split_docs(documents,chunk_size=256,chunk_overlap=20): # second try\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "ireland_docs = split_docs(ireland_documents)\n",
    "us_docs = split_docs(us_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51d04532-1ef5-47e5-89e2-42e3e412cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland_db = Qdrant.from_documents(ireland_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")\n",
    "us_db = Qdrant.from_documents(us_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985864dd-108e-4928-bf5b-30d76b137509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I have customized this in the function below\n",
    "def get_similiar_docs(query,k=2,score=False):\n",
    "  if score:\n",
    "    similar_docs = db.similarity_search_with_score(query,k=k)\n",
    "  else:\n",
    "    similar_docs = db.similarity_search(query,k=k)\n",
    "  return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f65f882b-ac03-47cd-bea3-c2ced022f543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 193\u001b[0m\n\u001b[1;32m    174\u001b[0m             warning_message \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem Message\u001b[39m\u001b[38;5;124m\"\u001b[39m, visible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;66;03m#file_upload = gr.File(label=\"Upload your documents here.\")                \u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m#Define examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Add examples to the interface\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gr.Examples(examples=examples, inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider])#, outputs=response_output)\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[43msubmit_button\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moci_llm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry_dropdown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dropdown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature_slider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p_slider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens_slider\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mresponse_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarning_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m clear_button\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mclear_input, \n\u001b[1;32m    197\u001b[0m                    inputs\u001b[38;5;241m=\u001b[39m[],  \u001b[38;5;66;03m# No input is needed for clearing\u001b[39;00m\n\u001b[1;32m    198\u001b[0m                    outputs\u001b[38;5;241m=\u001b[39m[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output, source_output, warning_message])\n\u001b[1;32m    199\u001b[0m flag_button\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mflag_response, \n\u001b[1;32m    200\u001b[0m                   inputs\u001b[38;5;241m=\u001b[39m[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output],  \n\u001b[1;32m    201\u001b[0m                   outputs\u001b[38;5;241m=\u001b[39m[warning_message])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/gradio/events.py:542\u001b[0m, in \u001b[0;36mEventListener._setup.<locals>.event_trigger\u001b[0;34m(block, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, every, trigger_mode, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _callback:\n\u001b[1;32m    541\u001b[0m     _callback(block)\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dependency(block, \u001b[43mdep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dep_index, fn, timer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/gradio/blocks.py:575\u001b[0m, in \u001b[0;36mBlockFunction.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id,\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets,\n\u001b[0;32m--> 575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs],\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs],\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjs,\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue,\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_name,\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscroll_to_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll_to_output,\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress,\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size,\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancels,\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes_generator,\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancel_function,\n\u001b[1;32m    589\u001b[0m         },\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollects_event_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollects_event_data,\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_after\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_after,\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_only_on_success\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_only_on_success,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_mode,\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_api\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_api,\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzerogpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_gpu,\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendered_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/gradio/blocks.py:575\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id,\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets,\n\u001b[0;32m--> 575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs],\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs],\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjs,\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue,\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_name,\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscroll_to_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll_to_output,\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress,\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size,\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancels,\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes_generator,\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancel_function,\n\u001b[1;32m    589\u001b[0m         },\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollects_event_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollects_event_data,\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_after\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_after,\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_only_on_success\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_only_on_success,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_mode,\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_api\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_api,\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzerogpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_gpu,\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendered_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "MODEL_ID_16K = 'cohere.command-r-16k'\n",
    "MODEL_ID_PLUS = 'cohere.command-r-plus'\n",
    "FLAGGED_DATA_PATH = \"flagged_data_folder/log.csv\"\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "}\n",
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\"\n",
    "\n",
    "# Utility function to get current timestamp\n",
    "def get_current_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, country=\"Irelnad\", model=\"Cohere 16k\", temperature=0, top_p=0.7, max_tokens=1000):    \n",
    "    if len(user_input) > 10:\n",
    "        prompts, source_name= setup_chat_context(user_input, country)\n",
    "        chat_response_text = process_chat_request(prompts, model, max_tokens, temperature, top_p)\n",
    "        return chat_response_text, source_name, \"Your feedback is much appreciated.\"\n",
    "    else:\n",
    "        return \"\", \"\", \"‚ö†Ô∏è Warning: Your input is too short.\"\n",
    "\n",
    "def get_similiar_docs(query, country, k=2):#,score=False):\n",
    "    similar_docs = ireland_db.similarity_search(query,k=k) if country == 'Ireland' else us_db.similarity_search(query,k=k)\n",
    "    return similar_docs     \n",
    "    \n",
    "def setup_chat_context(user_input, country):\n",
    "    similar_docs = get_similiar_docs(user_input, country)\n",
    "    concatenated_content, source_name, sourceLinks = \"\", \"\", \"\"\n",
    "    for document in similar_docs:\n",
    "        concatenated_content += document.page_content \n",
    "        source_name = document.metadata['source'].replace(\"new_data/\", \"\") # since the folder data/ was always present\n",
    "        #source_link = document.metadata[\"source\"]\n",
    "        #sourcePageNumber = int(document.metadata[\"page\"])\n",
    "        #sourcePageNumber = sourcePageNumber + 1\n",
    "#         if source_link not in unique_source_links:\n",
    "#           unique_source_links.add(source_link)  \n",
    "#           sourceLinks += \"<a href='\" + objectStorageLink + source_link + \"#page=\" + \"' target='_blank'>\" + source_link[source_link.rfind(\"/\")+1:] + \" (page \"+str(sourcePageNumber) + \")</a>\\n\"\n",
    "        #question\n",
    "        prompts = [f\"\"\"Go through this text: {concatenated_content}\\nQuestion: {user_input}\\nInstruction: {PROMPT_CONTEXT}\"\"\"]\n",
    "        prompts = \"\".join(prompts)\n",
    "        return prompts, source_name       \n",
    "        \n",
    "def process_chat_request(prompts, model, max_tokens, temperature, top_p):\n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "        chat_request = setup_chat_request(prompts, max_tokens, temperature, top_p)\n",
    "        chat_detail = setup_chat_detail(model, chat_request)\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        return chat_response_text        \n",
    "\n",
    "def setup_chat_request(prompts, max_tokens, temperature, top_p):\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = max_tokens \n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = temperature \n",
    "        chat_request.top_p = top_p \n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        #chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "        return chat_request\n",
    "        \n",
    "def setup_chat_detail(model, chat_request):\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        model_id = MODEL_ID_16K if model == \"Cohere 16k\" else MODEL_ID_PLUS\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id) \n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "        return chat_detail\n",
    "    \n",
    "# Specific UI Functions:   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    #return \"\", \"Cohere\", 0, 0.7, 1000, \"\"\n",
    "    return default_values[\"user_input\"], default_values[\"model_dropdown\"], default_values[\"temperature_slider\"], default_values[\"top_p_slider\"], default_values[\"max_tokens_slider\"], default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "# def handle_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback_type):\n",
    "#     feedback = 'liked' if feedback_type == 'positive' else 'disliked'\n",
    "#     timestamp = get_current_timestamp()\n",
    "#     with open(FLAGGED_DATA_CSV, \"a\", newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "#     return \"Thanks for your feedback.\"\n",
    "\n",
    "def handle_positive_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='liked'\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='disliked'\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"    \n",
    "                    This interface allows you to interact with different language models. Choose your settings and input your question below. You do not need to change any specifications.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3): #'scale' value should be an integer. Using 2.5 will cause issues.\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask the model what it thinks...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=8, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            #with gr.Row():\n",
    "                #country_checkbox = gr.CheckboxGroup([\"USA\", \"Ireland\"], label=\"Country\", info=\"Which countries should the regulations be applied for?\"),\n",
    "                country_dropdown = gr.Dropdown(choices=[\"USA\", \"Ireland\"], label=\"Country\", value=\"Ireland\", info=\"Which countries should the regulations be applied for?\"),\n",
    "                model_dropdown = gr.Dropdown(choices=[\"Cohere 16k\", \"Cohere Plus\"], label=\"Model\", value='Cohere 16k', info=\"Cohere 16k is a smaller-scale language model than Cohere Plus. While Cohere 16k offers high-quality responses, it might not possess the same level of sophistication and depth as Cohere Plus. \")\n",
    "                temperature_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0, label=\"Temperature\", info=\"The level of randomness used to generate the output text. A lower temperature means less random generations.\")\n",
    "                top_p_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.7, label=\"Top P\", info=\"This ensures that only the most likely tokens, with total probability mass of p, are considered for generation at each step.\")\n",
    "                max_tokens_slider = gr.Slider(minimum=100, maximum=4000, step=500, value=1000, label=\"Max Tokens\", info=\"The maximum number of output tokens that the model will generate for the response.\")\n",
    "                #country_checkbox = gr.CheckboxGroup([\"USA\", \"UK\"], label=\"Countries\", info=\"Which countries should the regulations be applied for?\"),\n",
    "            #with gr.Row():\n",
    "                warning_message = gr.Textbox(label=\"System Message\", visible=True, lines=1)\n",
    "                #file_upload = gr.File(label=\"Upload your documents here.\")                \n",
    "    \n",
    "    #Define examples\n",
    "    # examples = [\n",
    "    #     #{\"user_input\": \"Can a member give anything of value more than a hundred dollar?\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7, \"max_tokens\":1000}\n",
    "    #     #{\"user_input\": \"Explain the concept of deep learning.\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7}\n",
    "    #     [\"Tell me more about MCOAssitant.\",  \"Cohere 16k\",  0.2,  1, 800],\n",
    "    #     [\"Provide contact information for the Bank of England.\",  \"Cohere 16k\",  0,  1, 500],\n",
    "    #     [\"Can you summarize Private Securities Transactions of an Associated Person rules?\", \"Cohere 16k\",  0,  1, 1500],\n",
    "    #     [\"Can a member give anything of value more than a hundred dollar?\",  \"Cohere 16k\",  0,  0.9, 1000],\n",
    "    #     [\"Which obligations do not apply when successive personal transactions are carried out on behalf of a person? Make a list.\",  \"Cohere 16k\",  0,  1, 1000],\n",
    "    #     [\"What are examples of giving advice, or providing services, to an employer in connection with a group personal pension scheme or group stakeholderpension scheme? Make a list.\", \"Cohere 16k\", 0,  1, 1500]\n",
    "    # ]\n",
    "\n",
    "    # Add examples to the interface\n",
    "    # gr.Examples(examples=examples, inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider])#, outputs=response_output)\n",
    "        \n",
    "    \n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn= handle_positive_feedback,\n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback,\n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_tokens_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:13px; color:#3c3c3c\"> \n",
    "    Version 1.0.0.\n",
    "    </span> </div>\"\"\") \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True)#, server_port=44510)\n",
    "\n",
    "#### To do list\n",
    "# 1. Add country dropdown\n",
    "# 2. Make the checkbox work\n",
    "# 3. Add user drop down\n",
    "# 4. Add similary search for different countries\n",
    "# 5. Add country for like, dislike, clear button\n",
    "# 6. Add examples from their examples\n",
    "# 7. Check if country check box is compatible to have one value only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33287ea6-874d-46ec-ac14-ccae51ee7ab0",
   "metadata": {},
   "source": [
    "## 1.2. Previous Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94a68bdc-8a41-492a-b1e4-d8f3f0d78bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_context = \"You are an AI Assistant trained to give answers based only the information provided. Given only the above text provided and not prior knowledge, answer the query. If someone asks you a question and you don't know the answer, don't try to make up a response, simply say: I don't know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe5e104c-db47-4b4c-be91-5d4e4aacca7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n",
      "Running on public URL: https://f5354e95d0c9ecaa10.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f5354e95d0c9ecaa10.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/blocks.py\", line 1508, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/utils.py\", line 818, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_19405/375473604.py\", line 30, in oci_llm_model\n",
      "    similar_docs = get_similiar_docs(message)\n",
      "TypeError: get_similiar_docs() missing 1 required positional argument: 'country'\n"
     ]
    }
   ],
   "source": [
    "model_id_16k = 'cohere.command-r-16k'\n",
    "model_id_plus = 'cohere.command-r-plus'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "feedback = \"None\"\n",
    "\n",
    "# Define the path to flagged CSV file\n",
    "flagged_data_csv = \"flagged_data_folder/log.csv\"\n",
    "data_folder_path = \"data/\"\n",
    "\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "}\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, model=default_values['model_dropdown'], temperature=default_values['temperature_slider'] , top_p=default_values['top_p_slider'], max_tokens=default_values['max_tokens_slider']):\n",
    "\n",
    "    ############################################################################# OCI Function Starts Here\n",
    "    \n",
    "    if len(user_input) > 10:\n",
    "        warning_message = \"Your feedback is much appreciated.\"\n",
    "    \n",
    "        message = user_input\n",
    "        similar_docs = get_similiar_docs(message)\n",
    "        #similar_docs = get_similiar_docs_faiss(message)\n",
    "\n",
    "    # The output of sources is not totally desirable atm\n",
    "    #     context\n",
    "        concatenated_content = \"\"\n",
    "        sourceLinks = \"\"\n",
    "        #sourcePageNumber = \"\"\n",
    "        #unique_source_links = set() \n",
    "        for document in similar_docs:\n",
    "            concatenated_content += document.page_content \n",
    "            source_link = document.metadata[\"source\"]\n",
    "            #sourcePageNumber = int(document.metadata[\"page\"])\n",
    "            #sourcePageNumber = sourcePageNumber + 1\n",
    "    #         if source_link not in unique_source_links:\n",
    "    #           unique_source_links.add(source_link)  \n",
    "    #           sourceLinks += \"<a href='\" + objectStorageLink + source_link + \"#page=\" + \"' target='_blank'>\" + source_link[source_link.rfind(\"/\")+1:] + \" (page \"+str(sourcePageNumber) + \")</a>\\n\"\n",
    "\n",
    "        #context\n",
    "        source_name = \"\"\n",
    "        for document in similar_docs:\n",
    "            source_name = document.metadata['source'].replace(\"data/\", \"\") # since the folder data/ was always present\n",
    "\n",
    "        #question\n",
    "        prompt_token_input = prompt_context\n",
    "\n",
    "        prompts = [\n",
    "        f\"\"\"Please go through this text: {concatenated_content}\n",
    "        Question: {message}\n",
    "        Instruction: {prompt_token_input}\"\"\"]\n",
    "        prompts = \"\".join(prompts)\n",
    "\n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        #chat_request = oci.generative_ai_inference.models.GenericChatRequest() # using this result into an error, it might need a different authentication method\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = max_tokens #1000 #max_tokens\n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = temperature #temperature\n",
    "        chat_request.top_p = top_p # top_p\n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "        #chat_request.chat_history = oci.generative_ai_inference.models.CohereMessage(role=\"USER\", message=\"\")\n",
    "\n",
    "        # This part is redundant since the model keeps the history\n",
    "        # previous_chat_message = oci.generative_ai_inference.models.CohereMessage(role=\"USER\", message=previous_chat_message)\n",
    "        # previous_chat_reply = oci.generative_ai_inference.models.CohereMessage(role=\"CHATBOT\", message=previous_chat_reply)\n",
    "        # chat_request.chat_history = [previous_chat_message, previous_chat_reply]\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "\n",
    "        # if \"<compartment_ocid>\" in compartment_id:\n",
    "        #     print(\"ERROR:Please update your compartment id in target python file\")\n",
    "        #     quit()\n",
    "\n",
    "        # Selectiong the model\n",
    "        if model == \"Cohere 16k\":\n",
    "            chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_16k)\n",
    "        elif model == \"Cohere Plus\":\n",
    "            chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_plus)\n",
    "\n",
    "        #chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "        # result\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        #print(\"Extracted text:\", chat_response_text)\n",
    "        return chat_response_text, source_name, warning_message\n",
    "        #oci.util.to_dict(chat_response_text+' For more info check the below links: \\n'+sourceLinks)\n",
    "        \n",
    "    else: # if the user prompt's length is less than 10, just return the warning message and do not go into LLM model\n",
    "        chat_response_text = \"\"\n",
    "        source_name = \"\"\n",
    "        warning_message = \"‚ö†Ô∏è Warning: Your input is too short.\"\n",
    "        return chat_response_text, source_name, warning_message\n",
    "\n",
    "    ############################################################################# OCI Function Ends Here   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    #return \"\", \"Cohere\", 0, 0.7, 1000, \"\"\n",
    "    return default_values[\"user_input\"], default_values[\"model_dropdown\"], default_values[\"temperature_slider\"], default_values[\"top_p_slider\"], default_values[\"max_tokens_slider\"], default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "def handle_positive_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='liked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='disliked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    # Generate a timestamp for when the content was flagged\n",
    "    #timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Open the CSV file in append mode\n",
    "    \n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"    \n",
    "                    This interface allows you to interact with different language models. Choose your settings and input your question below. You do not need to change any specifications.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3): #'scale' value should be an integer. Using 2.5 will cause issues.\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask the model what it thinks...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=8, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            #with gr.Row():\n",
    "                model_dropdown = gr.Dropdown(choices=[\"Cohere 16k\", \"Cohere Plus\"], label=\"Model\", value='Cohere 16k', info=\"Cohere 16k is a smaller-scale language model than Cohere Plus. While Cohere 16k offers high-quality responses, it might not possess the same level of sophistication and depth as Cohere Plus. \")\n",
    "                temperature_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0, label=\"Temperature\", info=\"The level of randomness used to generate the output text. A lower temperature means less random generations.\")\n",
    "                top_p_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.7, label=\"Top P\", info=\"This ensures that only the most likely tokens, with total probability mass of p, are considered for generation at each step.\")\n",
    "                max_token_slider = gr.Slider(minimum=100, maximum=4000, step=500, value=1000, label=\"Max Tokens\", info=\"The maximum number of output tokens that the model will generate for the response.\")\n",
    "                #country_checkbox = gr.CheckboxGroup([\"USA\", \"UK\"], label=\"Countries\", info=\"Which countries should the regulations be applied for?\"),\n",
    "            #with gr.Row():\n",
    "                warning_message = gr.Textbox(label=\"System Message\", visible=True, lines=1)\n",
    "                #file_upload = gr.File(label=\"Upload your documents here.\")                \n",
    "    \n",
    "    #Define examples\n",
    "    examples = [\n",
    "        #{\"user_input\": \"Can a member give anything of value more than a hundred dollar?\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7, \"max_tokens\":1000}\n",
    "        #{\"user_input\": \"Explain the concept of deep learning.\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7}\n",
    "        [\"Tell me more about MCOAssitant.\",  \"Cohere 16k\",  0.2,  1, 800],\n",
    "        [\"Provide contact information for the Bank of England.\",  \"Cohere 16k\",  0,  1, 500],\n",
    "        [\"Can you summarize Private Securities Transactions of an Associated Person rules?\", \"Cohere 16k\",  0,  1, 1500],\n",
    "        [\"Can a member give anything of value more than a hundred dollar?\",  \"Cohere 16k\",  0,  0.9, 1000],\n",
    "        [\"Which obligations do not apply when successive personal transactions are carried out on behalf of a person? Make a list.\",  \"Cohere 16k\",  0,  1, 1000],\n",
    "        [\"What are examples of giving advice, or providing services, to an employer in connection with a group personal pension scheme or group stakeholderpension scheme? Make a list.\", \"Cohere 16k\", 0,  1, 1500]\n",
    "    ]\n",
    "\n",
    "    # Add examples to the interface\n",
    "    gr.Examples(examples=examples, inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider])#, outputs=response_output)\n",
    "\n",
    "        \n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"**The features that need to be added to the model in the later versions:**\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"\"\" \n",
    "            - The dropdown button of model selection actually works ‚úîÔ∏è v. 0.4.6.\n",
    "            - Add info for the parameters ‚úîÔ∏è v. 0.4.3. & 0.4.6. \n",
    "            - Add frequency penalty and max tokens ‚úîÔ∏è v. 0.4.4.\n",
    "            - Add examples ‚úîÔ∏è v. 0.4.5.\n",
    "            - Add a clear button ‚úîÔ∏è v. 0.4.2.\n",
    "            - Compare which models give more accurate answers (16k or Plus) ‚úîÔ∏è v. 0.4.6.\n",
    "            - The reference of the reponse is provided ‚úîÔ∏è v. 0.5.1.\n",
    "            - More efficient code ‚úîÔ∏è v. 0.4.7.\n",
    "            - Delete Source box with clear button ‚úîÔ∏èv. 0.5.2.\n",
    "            - More prompt engineering ‚úîÔ∏èv. 0.8.2.\n",
    "            - The user has the ability to add their own documents ‚úîÔ∏èv. 0.8.3.\n",
    "            - Add flag button ‚úîÔ∏èv. 0.6.4.\n",
    "\n",
    "            \"\"\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"\"\" \n",
    "            - Check for prompt injection\n",
    "            - Handle errors if any ‚úîÔ∏è v. 0.6.3. query length\n",
    "            - More descriptions for the app\n",
    "            - Add MCO logo \n",
    "            - Add more examples ‚úîÔ∏èv. 0.7.2.\n",
    "            - Add like and dislike button ‚úîÔ∏èv. 0.9.1.\n",
    "            - Show custom messages like flag saved ‚úîÔ∏èv. 0.7.1.\n",
    "            - Maybe tweak the source names ‚úîÔ∏èv. 0.7.3.\n",
    "            - Check why Cohere plus cannot respond well\n",
    "            - Add multiple sources if available\n",
    "            - Check if streaming works ‚ùå not feasible\n",
    "            - Add this to hugging face space ‚ùå not feasible\n",
    "            \"\"\")\n",
    "\n",
    "    # When the submit button is clicked, call the oci_llm_model function\n",
    "    # Include temperature_slider in the inputs\n",
    "    \n",
    "    #user_input.change(fn=check_input, inputs=[user_input], outputs=[warning_message])\n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn=handle_positive_feedback, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    #file_upload.change(fn=handle_file_upload, inputs=file_upload, outputs=warning_message)\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:13px; color:#3c3c3c\"> \n",
    "    Version 0.9.1.\n",
    "    </span> </div>\"\"\") \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True)#, server_port=44510)\n",
    "\n",
    "## test: Can a member give anything of value more than a hundred dollar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab5c42-86f2-49d5-93d6-169269bbc7c6",
   "metadata": {},
   "source": [
    "## 1.2.1. Previous Implemention with New Dataset ‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a36f87e7-0082-4e4c-b1b2-0c1f267da1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IRELAND_DIRECTORY = \"new_data/Ireland\"\n",
    "US_DIRECTORY = \"new_data/USA\"\n",
    "\n",
    "# PROMPT_CONTEXT = \"\"\"\n",
    "#     Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "#     ###\n",
    "#     Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "# \"\"\"\n",
    "\n",
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable. Consider the user's country whenever applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d2563e55-fbf5-4898-a7d2-5f3e58a6c090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  loader = MyDirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "ireland_documents = load_docs(IRELAND_DIRECTORY)\n",
    "us_documents = load_docs(US_DIRECTORY)\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "#def split_docs(documents,chunk_size=256,chunk_overlap=20): # second try\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "ireland_docs = split_docs(ireland_documents)\n",
    "us_docs = split_docs(us_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d9699bbe-ce25-4d8e-9766-c4496e285b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ireland_db = Qdrant.from_documents(ireland_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")\n",
    "us_db = Qdrant.from_documents(us_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99cb823a-6dab-40fb-aeba-ee0ef7dd55e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#message = 'Can I receive a gift of $50?'\n",
    "message = 'The contact details?'\n",
    "similar_docs = ireland_db.similarity_search(message,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "562243b6-20e0-4a16-bc40-ffced404f7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  \n",
      "  5. The Bank‚Äôs Support Structure  \n",
      "The Bank has put in place a number of supports to assist staff in complying with their ethical \n",
      "obligations, including those contained in this Policy. The Compliance Function, located in \n",
      "ORD, is available to assist with any queries. The contact details are set out below:  \n",
      "  \n",
      "compliance@centralbank.ie    \n",
      "  \n",
      "Staff may also wish to discuss these matters with the external Ethics Officer.  \n",
      "  \n",
      "6. Registers  \n",
      "  \n",
      "Divisional management will maintain the Register of Gifts and Business Hospitality for staff \n",
      "within their divi sions. GSD will maintain the Register of Gifts and Business Hospitality for \n",
      "members of the Senior Leadership Committees. The content of these registers will be reported \n",
      "periodically to the Compliance Function.  \n",
      "  \n",
      "The Compliance Function will also keep a re gister of gifts which have been surrendered to the \n",
      "Bank in accordance with the requirements of this Policy. The Compliance Function will manage\n",
      "new_data/Ireland/Central Bank of Ireland policy-on-receipt-of-business-hospitality-and-gifts.pdf\n",
      "Bank in accordance with the requirements of this Policy. The Compliance Function will manage \n",
      "these gifts on behalf of the Bank, including by disposal or raffle, if deemed appropriate.  \n",
      "  \n",
      "7. Breaches of the  Policy   \n",
      "  \n",
      "Staff should note that failure to adhere to the Bank‚Äôs rules on the acceptance of business \n",
      "hospitality and gifts may result in disciplinary action.\n",
      "new_data/Ireland/Central Bank of Ireland policy-on-receipt-of-business-hospitality-and-gifts.pdf\n"
     ]
    }
   ],
   "source": [
    "for document in similar_docs:\n",
    "    print(document.page_content)\n",
    "    print(document.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb21529e-ed83-4479-82dc-d595c5db0fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7914\n",
      "Running on public URL: https://d2bb3349bf7b3013a7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d2bb3349bf7b3013a7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### VERSION 2.0 ####\n",
    "import re\n",
    "\n",
    "model_id_16k = 'cohere.command-r-16k'\n",
    "model_id_plus = 'cohere.command-r-plus'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "feedback = \"None\"\n",
    "\n",
    "\n",
    "# Define the path to flagged CSV file\n",
    "flagged_data_csv = \"flagged_data_folder/log.csv\"\n",
    "data_folder_path = \"data/\"\n",
    "\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "}\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, country='Ireland', model=default_values['model_dropdown'], temperature=default_values['temperature_slider'] , top_p=default_values['top_p_slider'], max_tokens=default_values['max_tokens_slider']):\n",
    "\n",
    "    ############################################################################# OCI Function Starts Here\n",
    "    \n",
    "    if len(user_input) > 10:\n",
    "        warning_message = \"Your feedback is much appreciated.\"\n",
    "    \n",
    "        message = user_input\n",
    "        # change k placement later\n",
    "        similar_docs = ireland_db.similarity_search(message,k=2) if country == 'Ireland' else us_db.similarity_search(message,k=2)\n",
    "        #similar_docs = get_similiar_docs(message)\n",
    "        #similar_docs = get_similiar_docs_faiss(message)\n",
    "\n",
    "    # The output of sources is not totally desirable atm\n",
    "    #     context\n",
    "        concatenated_content = \"\"        \n",
    "        sourcePageNumber = \"\"\n",
    "        source = \"\"\n",
    "        source_name = \"\"\n",
    "        unique_source_set = set() \n",
    "        # sourceLinks = \"\"\n",
    "        # unique_source_links = set() \n",
    "        for document in similar_docs:\n",
    "            concatenated_content += document.page_content \n",
    "            #source_link = document.metadata[\"source\"]\n",
    "            file_pattern = r\"^new_data/(?:USA|Ireland)/?\"\n",
    "            source = re.sub(file_pattern, \"\",  document.metadata[\"source\"])\n",
    "            #source = document.metadata[\"source\"].replace(\"new_data/\", \"\").replace(\"Ireland/\",\"\")\n",
    "            if source.endswith(\".csv\"):\n",
    "                sourcePageNumber = 0\n",
    "            elif source.endswith(\".pdf\"):\n",
    "                sourcePageNumber = int(document.metadata[\"page\"])+1\n",
    "            # sourcePageNumber = int(document.metadata[\"page\"]) if document.metadata[\"page\"] else 0\n",
    "            # sourcePageNumber = sourcePageNumber + 1\n",
    "            if source_name not in unique_source_set:\n",
    "                source_name = \"File Name:\" + source + '\\n' + \"Page Number:\" + str(sourcePageNumber)\n",
    "                unique_source_set.add(source_name) \n",
    "                \n",
    "            # if source_link not in unique_source_links:\n",
    "                #unique_source_links.add(source_link)  \n",
    "                #sourceLinks += \"<a href='\" + objectStorageLink + source_link + \"#page=\" + \"' target='_blank'>\" + source_link[source_link.rfind(\"/\")+1:] + \" (page \"+str(sourcePageNumber) + \")</a>\\n\"\n",
    "\n",
    "        #source\n",
    "        # source_name = \"\"\n",
    "        # for document in similar_docs:\n",
    "        #     source_name = document.metadata['source'].replace(\"new_data/\", \"\") # since the folder data/ was always present\n",
    "\n",
    "        #question\n",
    "        prompt_token_input = prompt_context\n",
    "\n",
    "        # prompts = [\n",
    "        # f\"\"\"Please go through this text: {concatenated_content}\n",
    "        # Question: {message}\n",
    "        # Instruction: {prompt_token_input}\"\"\"]\n",
    "        # prompts = \"\".join(prompts)\n",
    "        \n",
    "        prompts = [\n",
    "        f\"\"\"Please go through this text: {concatenated_content}, the user's country is {country_dropdown}\n",
    "        Question: {message}\n",
    "        Instruction: {prompt_token_input}\"\"\"]\n",
    "        prompts = \"\".join(prompts)\n",
    "\n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        #chat_request = oci.generative_ai_inference.models.GenericChatRequest() # using this result into an error, it might need a different authentication method\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = max_tokens #1000 #max_tokens\n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = temperature #temperature\n",
    "        chat_request.top_p = top_p # top_p\n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "        #chat_request.chat_history = oci.generative_ai_inference.models.CohereMessage(role=\"USER\", message=\"\")\n",
    "\n",
    "        # This part is redundant since the model keeps the history\n",
    "        # previous_chat_message = oci.generative_ai_inference.models.CohereMessage(role=\"USER\", message=previous_chat_message)\n",
    "        # previous_chat_reply = oci.generative_ai_inference.models.CohereMessage(role=\"CHATBOT\", message=previous_chat_reply)\n",
    "        # chat_request.chat_history = [previous_chat_message, previous_chat_reply]\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "\n",
    "        # if \"<compartment_ocid>\" in compartment_id:\n",
    "        #     print(\"ERROR:Please update your compartment id in target python file\")\n",
    "        #     quit()\n",
    "\n",
    "        # Selectiong the model\n",
    "        if model == \"Cohere 16k\":\n",
    "            chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_16k)\n",
    "        elif model == \"Cohere Plus\":\n",
    "            chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_plus)\n",
    "\n",
    "        #chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "        # result\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        #print(\"Extracted text:\", chat_response_text)\n",
    "        return chat_response_text, source_name, warning_message\n",
    "        #oci.util.to_dict(chat_response_text+' For more info check the below links: \\n'+sourceLinks)\n",
    "        \n",
    "    else: # if the user prompt's length is less than 10, just return the warning message and do not go into LLM model\n",
    "        chat_response_text = \"\"\n",
    "        source_name = \"\"\n",
    "        warning_message = \"‚ö†Ô∏è Warning: Your input is too short.\"\n",
    "        return chat_response_text, source_name, warning_message\n",
    "\n",
    "    ############################################################################# OCI Function Ends Here   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    #return \"\", \"Cohere\", 0, 0.7, 1000, \"\"\n",
    "    return default_values[\"user_input\"], default_values[\"model_dropdown\"], default_values[\"temperature_slider\"], default_values[\"top_p_slider\"], default_values[\"max_tokens_slider\"], default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "def handle_positive_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='liked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='disliked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    # Generate a timestamp for when the content was flagged\n",
    "    #timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Open the CSV file in append mode\n",
    "    \n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='font-size:34px; color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"    \n",
    "                    This interface allows you to interact with different language models. Choose your settings and input your question below. You do not need to change any specifications.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3): #'scale' value should be an integer. Using 2.5 will cause issues.\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask the model what it thinks...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=8, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            #with gr.Row():\n",
    "                country_dropdown = gr.Dropdown(choices=[\"Ireland\", \"USA\"], label=\"Country\", value='Ireland', info=\"Which country should the regulation be applied to?\")\n",
    "                model_dropdown = gr.Dropdown(choices=[\"Cohere 16k\", \"Cohere Plus\"], label=\"Model\", value='Cohere 16k', info=\"Cohere 16k is a smaller-scale language model than Cohere Plus. While Cohere 16k offers high-quality responses, it might not possess the same level of sophistication and depth as Cohere Plus. \")\n",
    "                temperature_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0, label=\"Temperature\", info=\"The level of randomness used to generate the output text. A lower temperature means less random generations.\")\n",
    "                top_p_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.7, label=\"Top P\", info=\"This ensures that only the most likely tokens, with total probability mass of p, are considered for generation at each step.\")\n",
    "                max_token_slider = gr.Slider(minimum=100, maximum=4000, step=500, value=1000, label=\"Max Tokens\", info=\"The maximum number of output tokens that the model will generate for the response.\")\n",
    "                #country_checkbox = gr.CheckboxGroup([\"USA\", \"UK\"], label=\"Countries\", info=\"Which countries should the regulations be applied for?\"),\n",
    "            #with gr.Row():\n",
    "                warning_message = gr.Textbox(label=\"System Message\", visible=True, lines=1)\n",
    "                #file_upload = gr.File(label=\"Upload your documents here.\")                \n",
    "    \n",
    "    #Define examples\n",
    "    # examples = [\n",
    "    #     #{\"user_input\": \"Can a member give anything of value more than a hundred dollar?\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7, \"max_tokens\":1000}\n",
    "    #     #{\"user_input\": \"Explain the concept of deep learning.\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7}\n",
    "    #     [\"Tell me more about MCOAssitant.\",  \"Cohere 16k\",  0.2,  1, 800],\n",
    "    #     [\"Provide contact information for the Bank of England.\",  \"Cohere 16k\",  0,  1, 500],\n",
    "    #     [\"Can you summarize Private Securities Transactions of an Associated Person rules?\", \"Cohere 16k\",  0,  1, 1500],\n",
    "    #     [\"Can a member give anything of value more than a hundred dollar?\",  \"Cohere 16k\",  0,  0.9, 1000],\n",
    "    #     [\"Which obligations do not apply when successive personal transactions are carried out on behalf of a person? Make a list.\",  \"Cohere 16k\",  0,  1, 1000],\n",
    "    #     [\"What are examples of giving advice, or providing services, to an employer in connection with a group personal pension scheme or group stakeholderpension scheme? Make a list.\", \"Cohere 16k\", 0,  1, 1500]\n",
    "    # ]\n",
    "\n",
    "    # Add examples to the interface\n",
    "    # gr.Examples(examples=examples, inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider])#, outputs=response_output)\n",
    "\n",
    "        \n",
    "    # When the submit button is clicked, call the oci_llm_model function\n",
    "    # Include temperature_slider in the inputs\n",
    "    \n",
    "    #user_input.change(fn=check_input, inputs=[user_input], outputs=[warning_message])\n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, country_dropdown, model_dropdown, temperature_slider, top_p_slider, max_token_slider], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn=handle_positive_feedback, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    #file_upload.change(fn=handle_file_upload, inputs=file_upload, outputs=warning_message)\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:13px; color:#3c3c3c\"> \n",
    "    Version 0.9.1.\n",
    "    </span> </div>\"\"\") \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True)#, server_port=44510)\n",
    "\n",
    "## test: Can a member give anything of value more than a hundred dollar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283ba94-8923-483c-8c33-a6430d5010c7",
   "metadata": {},
   "source": [
    "### Concerns, Current Issues\n",
    "- When added the similarity search inside the loop, the model repeatition problem seems to be resolved. This could be due to lower number of k as well which I decreased from 4 to 2.\n",
    "- The model is able to respond based from the Excel file, however it is not able to comprehend the mixed logical questions, like adding two or more column values in a row alongside their column labels. It might give back false responses. Furthermore, the Excel source is never mentioned.\n",
    "- The model is somehow well able to read the table inside a document (pdf file) and  the column headers are properly rendered. (check further). However the table inside a table (csv, excel) is not properly read. Maybe converting the excel file to pdf would help?\n",
    "- Acronyms should be avoided both in questions and the document sources, unless they are well defined in the context.\n",
    "- It is better to state which MasterCard groups are allowed instead of resitricted since the model have tendendy to bring back the names of restricted secuirties as the answers, instead of taking this into account that they are forbidden.\n",
    "- We have to decide the file types (pdf, document, html, excel, even images) and check which ones are the common ones and the proper data cleaning each one needs. At the moment we are going to one-size-fits-all text parsing which which is not quiet tailored to each files nature. Based on my experience, it is better to have all the information/context in the same file formats as much as possible.\n",
    "- We also need to decide on the common user levels as well.\n",
    "- The more we need human intervention in cleaning and formatting the data, before feeding it to the model, the more error prone the knowledge bank would be. So we need to be able to automate this process as much as we can.\n",
    "- The source is almost never the restricted file, either because this is too small or either the first document is the first source.\n",
    "- As the number of clients and their respective client groups increases, the level of complexities arises, which can lead to slower database reading and eventually higher latency.\n",
    "\n",
    "\n",
    "### To do list\n",
    "1. Add country dropdown ‚úîÔ∏è\n",
    "2. Make the country work ‚úîÔ∏è\n",
    "3. Add similary search for different countries ‚úîÔ∏è\n",
    "4. Check about more options for metadata ‚úîÔ∏è\n",
    "5. Add default country ‚úîÔ∏è\n",
    "6. Make a csv file of their excel file and upload it ‚úîÔ∏è\n",
    "7. Hide database folder from the source ‚úîÔ∏è\n",
    "8. Page metadata is not working for csv file, add that restriction to the loop ‚úîÔ∏è\n",
    "9. Add client, company and user group ‚úîÔ∏è\n",
    "10. Better define k for similiar search ‚úîÔ∏è\n",
    "11. Omit model, top p and max tokens if they seem redundant ‚úîÔ∏è\n",
    "12. check model output or the regulations that doesn't apply in certain clients and countries ‚úîÔ∏è\n",
    "13. Chat request documents could be changed based on the client ‚úîÔ∏è\n",
    "14. Layer the database based on country and user level ‚úîÔ∏è\n",
    "15. Put resitriction for countries in a sepatarate file and upload them separately ‚úîÔ∏è\n",
    "16. Check if clear button properly works17. Check if clear button properly works ‚úîÔ∏è\n",
    "17. Check if like button properly works ‚úîÔ∏è\n",
    "18. Check if dislike button properly works ‚úîÔ∏è\n",
    "19. Check if flag button properly works ‚úîÔ∏è\n",
    "20. Add client and user group for like, dislike, clear button- Add client and user group for like, dislike, clear button ‚úîÔ∏è\n",
    "21. Add examples from their examples ‚úîÔ∏è\n",
    "22. Add error messages instead of system message ‚úîÔ∏è\n",
    "\n",
    "\n",
    "- Add color to the warning message - this didn't work\n",
    "- Check if country check box is compatible to have one value only  \n",
    "- Check out how frequency penalty works\n",
    "- Check if there are restriction in multipe boxes in Gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69220eb4-d169-4ee1-ab47-7877982648d0",
   "metadata": {},
   "source": [
    "## Model for Presentation ‚≠ê‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e6016cc-dc05-487d-825b-b478c78cfb24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CENTRAL_BANK_OF_IRELAND_DIRECTORY = \"new_data/central_bank_of_Ireland\"\n",
    "MASTERCARD_US_EMPLOYEE_DIRECTORY = \"new_data/mastercard/us_employee\"\n",
    "MASTERCARD_IRISH_EMPLOYEE_DIRECTORY = \"new_data/mastercard/irish_employee\"\n",
    "\n",
    "# PROMPT_CONTEXT = \"\"\"\n",
    "#     Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "#     ###\n",
    "#     Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "# \"\"\"\n",
    "\n",
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable. Consider the user's country whenever applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dd87155f-bd0f-467b-acf9-d608ecab1dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable. Consider the user's country whenever applicable.\n",
    "    If the response includes an 'X', it indicates that the corresponding column is applicable. Please interpret these 'X' marks as indicators of applicability.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8833d194-aa8f-4e6e-8cf1-79e65809c689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  loader = MyDirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "central_bank_of_ireland_documents = load_docs(CENTRAL_BANK_OF_IRELAND_DIRECTORY)\n",
    "mastercard_us_documents = load_docs(MASTERCARD_US_EMPLOYEE_DIRECTORY)\n",
    "mastercard_irish_documents = load_docs(MASTERCARD_IRISH_EMPLOYEE_DIRECTORY)\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "#def split_docs(documents,chunk_size=256,chunk_overlap=20): # second try\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "central_bank_of_ireland_split_docs = split_docs(central_bank_of_ireland_documents)\n",
    "mastercard_us_split_docs = split_docs(mastercard_us_documents)\n",
    "mastercard_irish_split_docs = split_docs(mastercard_irish_documents)\n",
    "\n",
    "central_bank_of_ireland_db = Qdrant.from_documents(central_bank_of_ireland_split_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")\n",
    "mastercard_us_db = Qdrant.from_documents(mastercard_us_split_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")\n",
    "mastercard_irish_db = Qdrant.from_documents(mastercard_irish_split_docs, embeddings, location=\":memory:\", collection_name=\"my_documents\", distance_func=\"Dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "49acf12a-a120-4991-be56-56641798dcea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:44510\n",
      "Running on public URL: https://dfccd96d35eb98eb22.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dfccd96d35eb98eb22.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### VERSION 2.0 ####\n",
    "import re\n",
    "\n",
    "model_id_16k = 'cohere.command-r-16k'\n",
    "model_id_plus = 'cohere.command-r-plus'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "feedback = \"None\"\n",
    "k = 2\n",
    "\n",
    "\n",
    "# Define the path to flagged CSV file\n",
    "flagged_data_csv = \"flagged_data_folder/log.csv\"\n",
    "data_folder_path = \"data/\"\n",
    "\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "    \"client_dropdown\":\"Mastercard\",\n",
    "    \"user_group_dropdown\":\"US Employee\"\n",
    "}\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, client, user_group):\n",
    "\n",
    "    ############################################################################# OCI Function Starts Here\n",
    "    \n",
    "    if len(user_input) > 10:\n",
    "        warning_message = \"Your feedback helps me learn and grow. Please let me know if my answer was helpful.\"\n",
    "    \n",
    "        message = user_input\n",
    "        # change k placement later\n",
    "        if client == 'Central Bank of Ireland' and user_group == 'Irish Employee':\n",
    "            similar_docs = central_bank_of_ireland_db.similarity_search(message,k=k)\n",
    "                \n",
    "        elif client == 'Mastercard' and user_group == 'US Employee':\n",
    "            similar_docs = mastercard_us_db.similarity_search(message,k=k) \n",
    "            \n",
    "        elif client == 'Mastercard' and user_group == 'Irish Employee':\n",
    "            similar_docs = mastercard_irish_db.similarity_search(message,k=k) \n",
    "        \n",
    "        else: #how about the model output?\n",
    "            warning_message = f\"{client} regulations don't apply for {user_group}s.\"\n",
    "            chat_response_text = \"\"\n",
    "            source_name = \"\"\n",
    "            raise gr.Error(f\"{client} regulations don't apply for {user_group}s.\")\n",
    "            return chat_response_text, source_name, warning_message\n",
    "        \n",
    "        concatenated_content = \"\"        \n",
    "        sourcePageNumber = \"\"\n",
    "        source = \"\"\n",
    "        source_name = \"\"\n",
    "        unique_source_set = set() \n",
    "        # sourceLinks = \"\"\n",
    "        # unique_source_links = set() \n",
    "        for document in similar_docs:\n",
    "            concatenated_content += document.page_content \n",
    "            file_pattern = r\"^new_data/\"\n",
    "            source = re.sub(file_pattern, \"\",  document.metadata[\"source\"])\n",
    "            if source.endswith(\".csv\"):\n",
    "                sourcePageNumber = 0\n",
    "            elif source.endswith(\".pdf\"):\n",
    "                sourcePageNumber = int(document.metadata[\"page\"]) + 1\n",
    "            if source_name not in unique_source_set:\n",
    "                source_name = \"üåé File Name: \" + source + '\\n' + \"üåè Page Number: \" + str(sourcePageNumber)\n",
    "                unique_source_set.add(source_name) \n",
    "                \n",
    "        #question\n",
    "        prompt_token_input = prompt_context\n",
    "\n",
    "        prompts = [\n",
    "        f\"\"\"Please go through this text: {concatenated_content}\n",
    "        Question: {message}\n",
    "        Instruction: {prompt_token_input}\"\"\"]\n",
    "        prompts = \"\".join(prompts)\n",
    "        \n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = 2000 #1000 #max_tokens\n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = 0 #temperature #temperature\n",
    "        chat_request.top_p = 1 # top_p\n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "\n",
    "\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_16k)\n",
    "\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "        # result\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        gr.Info(\"Please let me know if my answer was helpful by providing feedback.\")\n",
    "        return chat_response_text, source_name, warning_message\n",
    "\n",
    "        \n",
    "    else: # if the user prompt's length is less than 10, just return the warning message and do not go into LLM model\n",
    "        chat_response_text = \"\"\n",
    "        source_name = \"\"\n",
    "        warning_message = \"‚ö†Ô∏è Warning: Your question is too short.\"\n",
    "        raise gr.Error(\"‚ö†Ô∏èYour question is too short!\")\n",
    "        return chat_response_text, source_name, warning_message\n",
    "\n",
    "    ############################################################################# OCI Function Ends Here   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    return default_values[\"user_input\"], default_values[\"client_dropdown\"], default_values[\"user_group_dropdown\"],  default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "def handle_positive_feedback(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    feedback ='liked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Your feedback is valuable. I'll use it to improve my responses.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    feedback ='disliked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Your feedback is valuable. I'll use it to improve my responses.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    # Open the CSV file in append mode\n",
    "    feedback = 'flagged'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='font-size:34px; color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"<span style='color:#3D52D5'> MCOAssistant </span> is a large language model specializing in compliance best practices. It offers expert guidance on a wide range of regulations, policies, and industry standards. By providing clear and concise answers, ComplianceGPT empowers businesses to navigate complex compliance landscapes with confidence. \n",
    "    Whether you need to interpret regulations, assess risks, or develop compliance strategies, <span style='color:#3D52D5'> MCOAssistant </span> is your trusted AI companion.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"What is in your mind...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=4, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            client_dropdown = gr.Dropdown(choices=[\"Central Bank of Ireland\", \"Mastercard\"], label=\"Client Name\", value='Mastercard', info=\"This is for presentation purpose only. The client's name is not allowed to change by the client.\")\n",
    "            user_group_dropdown = gr.Dropdown(choices=[\"Irish Employee\", \"US Employee\"], label=\"User Group\", value='US Employee', info=\"Which user is asking the question. For presentation purpose only, it is not visible in production level.\")\n",
    "            warning_message = gr.Textbox(label=\"System Message\", lines=3, visible=True, placeholder= \"üìã For more detailed and informative answers, try asking longer questions. üìã If you don't know what to ask try one the examples below.\")   \n",
    "\n",
    "    \n",
    "    #Define examples\n",
    "    examples = [\n",
    "        [\"Can I receive a gift of $50?\",  \"Central Bank of Ireland\",  \"Irish Employee\"],\n",
    "        [\"What kind of a gift can I not receive?\",  \"Central Bank of Ireland\", \"Irish Employee\"],\n",
    "        [\"Who do I need to inform about my gift?\", \"Central Bank of Ireland\", \"Irish Employee\"],\n",
    "        [\"What are the approval levels to give a gift to A Government Official?\",  \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I give a gift to a government official of $49?\",  \"Mastercard\",  \"US Employee\"],\n",
    "        [\"Do I need approval to give a gift of $50 to a government official?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"What are the approval levels for a gift to a Non Government business partner?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"What are the pre-approval levels for gifts for a Commercial Business Partner?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I trade VICO?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I trade VICO?\", \"Mastercard\", \"Irish Employee\"],\n",
    "    ]\n",
    "    \n",
    "    #Add examples to the interface\n",
    "    gr.Examples(examples=examples, inputs=[user_input, client_dropdown, user_group_dropdown])#, outputs=response_output)\n",
    "\n",
    "    \n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, client_dropdown, user_group_dropdown], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, client_dropdown, user_group_dropdown, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn=handle_positive_feedback, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])    \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True, server_port=44510)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59efd07-dee9-4c4f-87ec-ca4e048fbb93",
   "metadata": {},
   "source": [
    "## Model with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3c0a85f7-c031-4e0f-a871-5cff4c320378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:44451\n",
      "Running on public URL: https://224226215a9a877d18.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://224226215a9a877d18.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### VERSION 2.0 ####\n",
    "import re\n",
    "\n",
    "model_id_16k = 'cohere.command-r-16k'\n",
    "model_id_plus = 'cohere.command-r-plus'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "feedback = \"None\"\n",
    "k = 2\n",
    "\n",
    "\n",
    "# Define the path to flagged CSV file\n",
    "flagged_data_csv = \"flagged_data_folder/log.csv\"\n",
    "data_folder_path = \"data/\"\n",
    "\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "    \"client_dropdown\":\"Mastercard\",\n",
    "    \"user_group_dropdown\":\"US Employee\"\n",
    "}\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, client, user_group):\n",
    "\n",
    "    ############################################################################# OCI Function Starts Here\n",
    "    \n",
    "    if len(user_input) > 10:\n",
    "        warning_message = \"Your feedback helps me learn and grow. Please let me know if my answer was helpful.\"\n",
    "    \n",
    "        message = user_input\n",
    "        # change k placement later\n",
    "        if client == 'Central Bank of Ireland' and user_group == 'Irish Employee':\n",
    "            similar_docs = central_bank_of_ireland_db.similarity_search(message,k=k)\n",
    "                \n",
    "        elif client == 'Mastercard' and user_group == 'US Employee':\n",
    "            similar_docs = mastercard_us_db.similarity_search(message,k=k) \n",
    "            \n",
    "        elif client == 'Mastercard' and user_group == 'Irish Employee':\n",
    "            similar_docs = mastercard_irish_db.similarity_search(message,k=k) \n",
    "        \n",
    "        else: #how about the model output?\n",
    "            warning_message = f\"{client} regulations don't apply for {user_group}s.\"\n",
    "            chat_response_text = \"\"\n",
    "            source_name = \"\"\n",
    "            raise gr.Error(f\"{client} regulations don't apply for {user_group}s.\")\n",
    "            return chat_response_text, source_name, warning_message\n",
    "        \n",
    "        concatenated_content = \"\"        \n",
    "        sourcePageNumber = \"\"\n",
    "        source = \"\"\n",
    "        source_name = \"\"\n",
    "        unique_source_set = set() \n",
    "        # sourceLinks = \"\"\n",
    "        # unique_source_links = set() \n",
    "        source_link = \"<a href='https:/google.com/' >Visit google.com!</a>\"\n",
    "        for document in similar_docs:\n",
    "            concatenated_content += document.page_content \n",
    "            file_pattern = r\"^new_data/\"\n",
    "            source = re.sub(file_pattern, \"\",  document.metadata[\"source\"])\n",
    "            if source.endswith(\".csv\"):\n",
    "                sourcePageNumber = 0\n",
    "            elif source.endswith(\".pdf\"):\n",
    "                sourcePageNumber = int(document.metadata[\"page\"]) + 1\n",
    "            if source_name not in unique_source_set:\n",
    "                source_name = \"üåé File Name: \" + source + '\\n' + \"üåè Page Number: \" + str(sourcePageNumber) \n",
    "                unique_source_set.add(source_name)                 \n",
    "                \n",
    "        #question\n",
    "        prompt_token_input = prompt_context\n",
    "\n",
    "        prompts = [\n",
    "        f\"\"\"Please go through this text: {concatenated_content}\n",
    "        Question: {message}\n",
    "        Instruction: {prompt_token_input}\"\"\"]\n",
    "        prompts = \"\".join(prompts)\n",
    "        \n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = 2000 #1000 #max_tokens\n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = 0 #temperature #temperature\n",
    "        chat_request.top_p = 1 # top_p\n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "\n",
    "\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id_16k)\n",
    "\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "        # result\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        gr.Info(\"Please let me know if my answer was helpful by providing feedback.\")\n",
    "        return chat_response_text, source_name, warning_message, source_link\n",
    "\n",
    "        \n",
    "    else: # if the user prompt's length is less than 10, just return the warning message and do not go into LLM model\n",
    "        chat_response_text = \"\"\n",
    "        source_name = \"\"\n",
    "        warning_message = \"‚ö†Ô∏è Warning: Your question is too short.\"\n",
    "        source_link = \"\"\n",
    "        raise gr.Error(\"‚ö†Ô∏èYour question is too short!\")\n",
    "        return chat_response_text, source_name, warning_message, source_link\n",
    "\n",
    "    ############################################################################# OCI Function Ends Here   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    return default_values[\"user_input\"], default_values[\"client_dropdown\"], default_values[\"user_group_dropdown\"],  default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "def handle_positive_feedback(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    feedback ='liked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Your feedback is valuable. I'll use it to improve my responses.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    feedback ='disliked'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Your feedback is valuable. I'll use it to improve my responses.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, client_dropdown, user_group_dropdown, response_output):\n",
    "    # Open the CSV file in append mode\n",
    "    feedback = 'flagged'\n",
    "    with open(flagged_data_csv, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, client_dropdown, user_group_dropdown, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='font-size:34px; color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"<span style='color:#3D52D5'> MCOAssistant </span> is a large language model specializing in compliance best practices. It offers expert guidance on a wide range of regulations, policies, and industry standards. By providing clear and concise answers, ComplianceGPT empowers businesses to navigate complex compliance landscapes with confidence. \n",
    "    Whether you need to interpret regulations, assess risks, or develop compliance strategies, <span style='color:#3D52D5'> MCOAssistant </span> is your trusted AI companion.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"What is in your mind...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=4, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            gr.HTML(source_link)\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            client_dropdown = gr.Dropdown(choices=[\"Central Bank of Ireland\", \"Mastercard\"], label=\"Client Name\", value='Mastercard', info=\"This is for presentation purpose only. The client's name is not allowed to change by the client.\")\n",
    "            user_group_dropdown = gr.Dropdown(choices=[\"Irish Employee\", \"US Employee\"], label=\"User Group\", value='US Employee', info=\"Which user is asking the question. For presentation purpose only, it is not visible in production level.\")\n",
    "            warning_message = gr.Textbox(label=\"System Message\", lines=3, visible=True, placeholder= \"üìã For more detailed and informative answers, try asking longer questions. üìã If you don't know what to ask try one the examples below.\")   \n",
    "\n",
    "    \n",
    "    #Define examples\n",
    "    examples = [\n",
    "        [\"Can I receive a gift of $50?\",  \"Central Bank of Ireland\",  \"Irish Employee\"],\n",
    "        [\"What kind of a gift can I not receive?\",  \"Central Bank of Ireland\", \"Irish Employee\"],\n",
    "        [\"Who do I need to inform about my gift?\", \"Central Bank of Ireland\", \"Irish Employee\"],\n",
    "        [\"What are the approval levels to give a gift to A Government Official?\",  \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I give a gift to a government official of $49?\",  \"Mastercard\",  \"US Employee\"],\n",
    "        [\"Do I need approval to give a gift of $50 to a government official?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"What are the approval levels for a gift to a Non Government business partner?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"What are the pre-approval levels for gifts for a Commercial Business Partner?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I trade VICO?\", \"Mastercard\", \"US Employee\"],\n",
    "        [\"Can I trade VICO?\", \"Mastercard\", \"Irish Employee\"],\n",
    "    ]\n",
    "    \n",
    "    #Add examples to the interface\n",
    "    gr.Examples(examples=examples, inputs=[user_input, client_dropdown, user_group_dropdown])#, outputs=response_output)\n",
    "\n",
    "    \n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, client_dropdown, user_group_dropdown], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, client_dropdown, user_group_dropdown, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn=handle_positive_feedback, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback, \n",
    "                      inputs=[user_input, client_dropdown, user_group_dropdown, response_output],  \n",
    "                      outputs=[warning_message])    \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True, server_port=44451)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b10bd-fb67-40a6-ac9b-6f6852e73a6e",
   "metadata": {},
   "source": [
    "## 1.3. Refined Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbdfad0a-e7f3-4468-aa58-e44de200f00e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on public URL: https://33b8d99a41539cb4c8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://33b8d99a41539cb4c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Constants\n",
    "MODEL_ID_16k = 'cohere.command-r-16k'\n",
    "MODEL_ID_PLUS = 'cohere.command-r-plus'\n",
    "FLAGGED_DATA_PATH = \"flagged_data_folder/log.csv\"\n",
    "default_values = {\n",
    "    \"user_input\": \"\",\n",
    "    \"model_dropdown\": \"Cohere 16k\",\n",
    "    \"temperature_slider\": 0,\n",
    "    \"top_p_slider\": 0.7,\n",
    "    \"max_tokens_slider\": 1000,\n",
    "    \"response_output\": \"\",\n",
    "    \"source_output\": \"\",\n",
    "    \"warning_message\":\"\",\n",
    "}\n",
    "PROMPT_CONTEXT = \"\"\"\n",
    "    Answer the questions based on the context below. You will use your knowledge base for reliable sources to provide context and legal citations where applicable.\n",
    "    ###\n",
    "    Context: Your an agent with a vast knowledge base and expertise in business conduct. You can answer your questions on a wide range of topics related to ethical practices, legal regulations, and best practices in the business world.\n",
    "\"\"\"\n",
    "\n",
    "# Utility function to get current timestamp\n",
    "def get_current_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Step 1: Set up the LLM Model\n",
    "def oci_llm_model(user_input, model=default_values['model_dropdown'], temperature=default_values['temperature_slider'] , top_p=default_values['top_p_slider'], max_tokens=default_values['max_tokens_slider']):\n",
    "    \n",
    "    if len(user_input) > 10:\n",
    "        prompts, source_name= setup_chat_context(user_input)\n",
    "        chat_response_text = process_chat_request(prompts, model, max_tokens, temperature, top_p)\n",
    "        return chat_response_text, source_name, \"Your feedback is much appreciated.\"\n",
    "    else:\n",
    "        return \"\", \"\", \"‚ö†Ô∏è Warning: Your input is too short.\"\n",
    "    \n",
    "def setup_chat_context(user_input):\n",
    "    similar_docs = get_similiar_docs(user_input)\n",
    "    concatenated_content, source_name, sourceLinks = \"\", \"\", \"\"\n",
    "    for document in similar_docs:\n",
    "        concatenated_content += document.page_content \n",
    "        source_name = document.metadata['source'].replace(\"data/\", \"\") # since the folder data/ was always present\n",
    "        #source_link = document.metadata[\"source\"]\n",
    "        #sourcePageNumber = int(document.metadata[\"page\"])\n",
    "        #sourcePageNumber = sourcePageNumber + 1\n",
    "#         if source_link not in unique_source_links:\n",
    "#           unique_source_links.add(source_link)  \n",
    "#           sourceLinks += \"<a href='\" + objectStorageLink + source_link + \"#page=\" + \"' target='_blank'>\" + source_link[source_link.rfind(\"/\")+1:] + \" (page \"+str(sourcePageNumber) + \")</a>\\n\"\n",
    "        #question\n",
    "        prompts = [f\"\"\"Please go through this text: {concatenated_content}\\nQuestion: {user_input}\\nInstruction: {PROMPT_CONTEXT}\"\"\"]\n",
    "        prompts = \"\".join(prompt s)\n",
    "        return prompts, source_name       \n",
    "        \n",
    "def process_chat_request(prompts, model, max_tokens, temperature, top_p):\n",
    "        #prompt template\n",
    "        generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={},signer=oci_signer, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "        chat_request = setup_chat_request(prompts, max_tokens, temperature, top_p)\n",
    "        chat_detail = setup_chat_detail(model, chat_request)\n",
    "        chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "        data = chat_response.data\n",
    "        chat_response_data = data.chat_response\n",
    "        chat_response_text = chat_response_data.text\n",
    "        return chat_response_text        \n",
    "\n",
    "def setup_chat_request(prompts, max_tokens, temperature, top_p):\n",
    "        chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "        chat_request.message = prompts\n",
    "        chat_request.max_tokens = max_tokens \n",
    "        chat_request.is_stream = False # True results to an error\n",
    "        chat_request.temperature = temperature \n",
    "        chat_request.top_p = top_p \n",
    "        #chat_request.top_k = 10 #top_k # Only support topK within [0, 500]\n",
    "        #chat_request.frequency_penalty = 0 #frequency_penalty\n",
    "\n",
    "        chat_request.documents = [\n",
    "        {\n",
    "            \"title\": \"Tell more about MCOAssistant\",\n",
    "            \"snippet\": \"The primary objective of this integration is to advance the solution‚Äôs capabilities in delivering personalised user interactions and efficient data retrieval directly tailored to user needs. Specifically, not only is the ambition to make it easier to understand how to use the product but to also improve policy-related data retrieval for users of MCO. This AI is designed to assist financial services firms in effectively managing and mitigating compliance risk by providing real-time insights into relevant business rules and regulations. It should also be able to answer financial questions within the context of these regulations.\",\n",
    "            \"website\": \"https://mco.mycomplianceoffice.com/\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Stated\",\n",
    "        #     \"snippet\": \"FINRA's rules and guidance in the US strive to protect investors and ensure the integrity of today's rapidly evolving market.\",\n",
    "        #     \"website\": \"https://www.finra.org/rules-guidance/rulebooks/finra-rules\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"title\": \"More rules and regularation information in the United Kingdom\",\n",
    "        #     \"snippet\": \"This Handbook contains the complete record of Financial Conduct Authority (FCA) Legal Instruments.\",\n",
    "        #     \"website\": \"http://www.handbook.fca.org.uk/\"\n",
    "        # }\n",
    "        ]\n",
    "        return chat_request\n",
    "        \n",
    "def setup_chat_detail(model, chat_request):\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        model_id = MODEL_ID_16K if model == \"Cohere 16k\" else MODEL_ID_PLUS\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id) #‚úîÔ∏è\n",
    "        chat_detail.compartment_id = compartment_id\n",
    "        chat_detail.chat_request = chat_request\n",
    "        return chat_detail\n",
    "    \n",
    "# Specific UI Functions:   \n",
    "    \n",
    "def clear_input():\n",
    "    # Return an empty string to clear the user_input Textbox\n",
    "    #return \"\", \"Cohere\", 0, 0.7, 1000, \"\"\n",
    "    return default_values[\"user_input\"], default_values[\"model_dropdown\"], default_values[\"temperature_slider\"], default_values[\"top_p_slider\"], default_values[\"max_tokens_slider\"], default_values[\"response_output\"], default_values['source_output'], default_values[\"warning_message\"]\n",
    "\n",
    "# def handle_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback_type):\n",
    "#     feedback = 'liked' if feedback_type == 'positive' else 'disliked'\n",
    "#     timestamp = get_current_timestamp()\n",
    "#     with open(FLAGGED_DATA_CSV, \"a\", newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "#     return \"Thanks for your feedback.\"\n",
    "\n",
    "def handle_positive_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='liked'\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def handle_negative_feedback(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    feedback ='disliked'\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Thanks for your feedback.\"\n",
    "    return warning_message\n",
    "\n",
    "def flag_response(user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output):\n",
    "    timestamp = get_current_timestamp()\n",
    "    with open(FLAGGED_DATA_PATH, \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the timestamp, user input, and model response to the CSV file\n",
    "        writer.writerow([timestamp, user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, feedback])\n",
    "    warning_message = \"Content flagged for review.\"\n",
    "    return warning_message\n",
    "\n",
    "\n",
    "# Step 2: Set up Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app: #theme=gr.themes.Soft()\n",
    "    gr.Markdown(\"# <span style='color:#29339B'> MCO Assistant </span>\")\n",
    "    gr.Markdown(\"\"\"    \n",
    "                    This interface allows you to interact with different language models. Choose your settings and input your question below. You do not need to change any specifications.\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3): #'scale' value should be an integer. Using 2.5 will cause issues.\n",
    "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask the model what it thinks...\", lines=3 ,show_copy_button=False)\n",
    "                \n",
    "            with gr.Row():\n",
    "                submit_button = gr.Button(\"‚úîÔ∏è Submit\")\n",
    "                clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "                flag_button = gr.Button(\"üö© Flag\")\n",
    "            response_output = gr.Textbox(label=\"Model Response\", lines=8, show_copy_button=True, placeholder=\"The model's response will be displayed here...\")\n",
    "            source_output = gr.Textbox(label=\"Source\", lines=1, show_copy_button=True, placeholder=\"The reponse's source will be displayed here...\")\n",
    "            with gr.Row():\n",
    "                like_button = gr.Button(\"üëç Like\")\n",
    "                dislike_button = gr.Button(\"üëé Dislike\")\n",
    "        with gr.Column(scale=1):\n",
    "            #with gr.Row():\n",
    "                model_dropdown = gr.Dropdown(choices=[\"Cohere 16k\", \"Cohere Plus\"], label=\"Model\", value='Cohere 16k', info=\"Cohere 16k is a smaller-scale language model than Cohere Plus. While Cohere 16k offers high-quality responses, it might not possess the same level of sophistication and depth as Cohere Plus. \")\n",
    "                temperature_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0, label=\"Temperature\", info=\"The level of randomness used to generate the output text. A lower temperature means less random generations.\")\n",
    "                top_p_slider = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.7, label=\"Top P\", info=\"This ensures that only the most likely tokens, with total probability mass of p, are considered for generation at each step.\")\n",
    "                max_token_slider = gr.Slider(minimum=100, maximum=4000, step=500, value=1000, label=\"Max Tokens\", info=\"The maximum number of output tokens that the model will generate for the response.\")\n",
    "                #country_checkbox = gr.CheckboxGroup([\"USA\", \"UK\"], label=\"Countries\", info=\"Which countries should the regulations be applied for?\"),\n",
    "            #with gr.Row():\n",
    "                warning_message = gr.Textbox(label=\"System Message\", visible=True, lines=1)\n",
    "                #file_upload = gr.File(label=\"Upload your documents here.\")                \n",
    "    \n",
    "    #Define examples\n",
    "    examples = [\n",
    "        #{\"user_input\": \"Can a member give anything of value more than a hundred dollar?\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7, \"max_tokens\":1000}\n",
    "        #{\"user_input\": \"Explain the concept of deep learning.\", \"model\": \"Cohere\", \"temperature\": 0.5, \"top_p\": 0.7}\n",
    "        [\"Tell me more about MCOAssitant.\",  \"Cohere 16k\",  0.2,  1, 800],\n",
    "        [\"Provide contact information for the Bank of England.\",  \"Cohere 16k\",  0,  1, 500],\n",
    "        [\"Can you summarize Private Securities Transactions of an Associated Person rules?\", \"Cohere 16k\",  0,  1, 1500],\n",
    "        [\"Can a member give anything of value more than a hundred dollar?\",  \"Cohere 16k\",  0,  0.9, 1000],\n",
    "        [\"Which obligations do not apply when successive personal transactions are carried out on behalf of a person? Make a list.\",  \"Cohere 16k\",  0,  1, 1000],\n",
    "        [\"What are examples of giving advice, or providing services, to an employer in connection with a group personal pension scheme or group stakeholderpension scheme? Make a list.\", \"Cohere 16k\", 0,  1, 1500]\n",
    "    ]\n",
    "\n",
    "    # Add examples to the interface\n",
    "    gr.Examples(examples=examples, inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider])#, outputs=response_output)\n",
    "\n",
    "        \n",
    "    \n",
    "    submit_button.click(fn=oci_llm_model, \n",
    "                        inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider], \n",
    "                        outputs=[response_output, source_output, warning_message])\n",
    "    clear_button.click(fn=clear_input, \n",
    "                       inputs=[],  # No input is needed for clearing\n",
    "                       outputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output, source_output, warning_message])\n",
    "    flag_button.click(fn=flag_response, \n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    like_button.click(fn= handle_positive_feedback,\n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    dislike_button.click(fn=handle_negative_feedback,\n",
    "                      inputs=[user_input, model_dropdown, temperature_slider, top_p_slider, max_token_slider, response_output],  \n",
    "                      outputs=[warning_message])\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:13px; color:#3c3c3c\"> \n",
    "    Version 1.0.0.\n",
    "    </span> </div>\"\"\") \n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style=\"text-align:center;\">\n",
    "    <span style=\"font-size:11px; color:#3c3c3c\"> \n",
    "    Note: While the model strives to be accurate and helpful, it is not a substitute for legal advice.  It's always recommended to consult with a qualified attorney for specific legal issues. \n",
    "    </span> </div>\"\"\")\n",
    "\n",
    "# Step 3: Launch the interface\n",
    "app.launch(share=True)#, server_port=44510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e9ef1-c59b-4566-b1e8-069709ce8da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea7682-66e8-474d-b0ba-4067e4fd7b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
